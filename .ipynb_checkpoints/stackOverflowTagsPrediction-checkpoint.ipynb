{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7306edd150564f598b45a506ca907307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7196138), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0114cfeea0b6447abae3d3d12d486940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2166270), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5578f2d18c4ff28948e6a0ecee5d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1041379), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518aa64464864331b17e804901f8c976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from common.download_utils import download_week1_resources\n",
    "\n",
    "#download_week1_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anmolnarang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anmolnarang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#from grader import Grader\n",
    "#grader = Grader()\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval)\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('data/train.tsv')\n",
    "validation = read_data('data/validation.tsv')\n",
    "test = pd.read_csv('data/test.tsv', sep='\\t')\n",
    "#print(train)\n",
    "\n",
    "#train.head()\n",
    "\n",
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "X_test = test['title'].values\n",
    "\n",
    "\n",
    "#tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "#tokens=tokenizer.tokenize(text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(X_train[0])\n",
    "X_train_tokenized=[]\n",
    "for train_data in X_train:\n",
    "    X_train_tokenized.append(text_prepare(train_data))\n",
    "\n",
    "#print(X_train_tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tokenized=[]\n",
    "for val_data in X_val:\n",
    "    X_val_tokenized.append(text_prepare(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "    answers = [\"sql server equivalent excels choose function\", \n",
    "               \"free c++ memory vectorint arr\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        temp=text_prepare(ex)\n",
    "        print(temp)\n",
    "       \n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sql', 'server', 'equivalent', 'excel', 'choose', 'function']\n",
      "['free', 'c++', 'memory', 'vector', 'int', 'arr']\n",
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_text_prepare())\n",
    "#print(X_train_tokenized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REPLACE_BY_SPACE_RE ='[/(){}\\[\\]\\|@,;?]'\n",
    "BAD_SYMBOLS_RE = '[^0-9a-z #+_]'\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE,\" \",text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    \n",
    "    text = re.sub(BAD_SYMBOLS_RE,\"\",text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    \n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    text=tokenizer.tokenize(text);\n",
    "    \n",
    "   # text = word_tokenize(text) \n",
    "     # delete stopwords from text\n",
    "    \n",
    "    filtered_sentence = [] \n",
    "    for w in text: \n",
    "        if w not in STOPWORDS: \n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
