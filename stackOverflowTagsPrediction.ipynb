{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7306edd150564f598b45a506ca907307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7196138), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0114cfeea0b6447abae3d3d12d486940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2166270), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5578f2d18c4ff28948e6a0ecee5d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1041379), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518aa64464864331b17e804901f8c976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from common.download_utils import download_week1_resources\n",
    "\n",
    "#download_week1_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anmolnarang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anmolnarang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#from grader import Grader\n",
    "#grader = Grader()\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval)\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('data/train.tsv')\n",
    "validation = read_data('data/validation.tsv')\n",
    "test = pd.read_csv('data/test.tsv', sep='\\t')\n",
    "#print(train)\n",
    "\n",
    "#train.head()\n",
    "\n",
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "X_test = test['title'].values\n",
    "\n",
    "\n",
    "#tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "#tokens=tokenizer.tokenize(text);REPLACE_BY_SPACE_RE ='[/(){}\\[\\]\\|@,;?]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = '[^0-9a-z #+_]'\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE,\" \",text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    \n",
    "    text = re.sub(BAD_SYMBOLS_RE,\"\",text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    \n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    text=tokenizer.tokenize(text);\n",
    "    \n",
    "   # text = word_tokenize(text) \n",
    "     # delete stopwords from text\n",
    "    \n",
    "    filtered_sentence =\"\" \n",
    "    for w in text: \n",
    "        if w not in STOPWORDS: \n",
    "            filtered_sentence += w+\" \"\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#print(X_train[0])\n",
    "X_train_filtered=[]\n",
    "for train_data in X_train:\n",
    "    X_train_filtered.append(text_prepare(train_data))\n",
    "\n",
    "X_train=X_train_filtered\n",
    "#print(X_train_filtered[0])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_val_filtered=[]\n",
    "for val_data in X_val:\n",
    "    X_val_filtered.append(text_prepare(val_data))\n",
    "X_val=X_val_filtered\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_questions = []\n",
    "for line in open('data/text_prepare_tests.tsv', encoding='utf-8'):\n",
    "    line = text_prepare(line.strip())\n",
    "    prepared_questions.append(line)\n",
    "text_prepare_results = '\\n'.join(prepared_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [text_prepare(x) for x in X_train]\n",
    "X_val = [text_prepare(x) for x in X_val]\n",
    "X_test = [text_prepare(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'draw stacked dotplot r '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all tags from train corpus with their counts.\n",
    "tags_counts = {}\n",
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "\n",
    "for index in  range(len(X_train)):\n",
    "    words=X_train[index].split(' ')\n",
    "    for word in words:\n",
    "        if word in words_counts:\n",
    "            words_counts[word]+=1\n",
    "        else:\n",
    "            words_counts[word]=1\n",
    "#print(words_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tags in y_train:\n",
    "    for tag in tags:\n",
    "        if tag in tags_counts:\n",
    "            tags_counts[tag] += 1\n",
    "        else:\n",
    "            tags_counts[tag] = 1\n",
    "\n",
    "#print(tags_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31393"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tags_counts))\n",
    "len(words_counts)\n",
    "#print(test_text_prepare())\n",
    "#print(X_train_tokenized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_tags = sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)[:10000]\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:5000]\n",
    "#print(most_common_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_indexs():\n",
    "    key_indexes={}\n",
    "    index=0\n",
    "    for key,values in most_common_words:\n",
    "        key_indexes[key]=index\n",
    "        index+=1\n",
    "    return key_indexes\n",
    "#print(key_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\n",
    "WORDS_TO_INDEX = words_indexs() ####### YOUR CODE HERE #######\n",
    "#INDEX_TO_WORDS = ####### YOUR CODE HERE #######\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "\n",
    "\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        dict_size: size of the dictionary\n",
    "        \n",
    "        return a vecto\n",
    "from scipy import sparse as sp_sparser which is a bag-of-words representation of 'text'\n",
    "    \"\"\"\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    words=text.split(' ')\n",
    "    for word in words:\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]]=1\n",
    "\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (100000, 5000)\n",
      "X_val shape  (30000, 5000)\n",
      "X_test shape  (20000, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_mybag[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test â€” samples        \n",
    "        return TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "    \n",
    "    tfidf=TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2), token_pattern= '(\\S+)')\n",
    "\n",
    "    X_train=tfidf.fit_transform(X_train);\n",
    "    X_val=tfidf.fit_transform(X_val);\n",
    "    X_test=tfidf.fit_transform(X_test);\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    return X_train, X_val, X_test, tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "if 'c++' in tfidf_vocab:\n",
    "    print('true')\n",
    "if 'c#' in tfidf_vocab:\n",
    "    print('c#')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
